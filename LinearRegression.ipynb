{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdhBylVi/qFBNkaGOnFy1g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomouvic/SENG474/blob/main/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression with One Variable\n",
        "\n",
        "**Example:** Housing Prices (Portland, OR)\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/thomouvic/SENG474/raw/main/images/housing.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "**Supervised Learning:**\n",
        "Given the \"right answer\" for each example in data (training instance), predict real-valued output (**regression**), or discrete-valued output (**classification**). \n",
        "\n",
        "**Formally:**\n",
        "\n",
        "Training set:\n",
        "\n",
        "| size (x) | price (y)  |\n",
        "|--|--|\n",
        "| 2104 | 460 |\n",
        "| 1416 | 232 |\n",
        "| 1534 | 315 |\n",
        "\n",
        "**Notation:**\n",
        "\n",
        "$m$: number of training examples (instances)\n",
        "\n",
        "$x$: input variable/feature\n",
        "\n",
        "$y$: output variable / target variable / label variable\n",
        "\n",
        "$(x,y)$: a generic training example\n",
        "\n",
        "$\\left( x^{(i)}, y^{(i)} \\right)$: the i-th training example.\n",
        "\n",
        "\n",
        "**E.g.**\n",
        "\n",
        "$x^{(1)} = 2104$\n",
        "\n",
        "$x^{(2)} = 1416$\n",
        "\n",
        "$y^{(1)} = 460$\n",
        "\n",
        "$y^{(2)} = 232$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/thomouvic/SENG474/raw/main/images/hypothesis.png\" width=\"300\"/>\n",
        "</div>\n",
        "\n",
        "$h$ maps from x's to y's.\n",
        "\n",
        "How to represent $h$?\n",
        "\n",
        "$$\n",
        "h_{\\theta}(x) = \\theta_0 + \\theta_1 x\n",
        "$$\n",
        "\n",
        "$\\theta$ is the parameter vector represented as a *column matrix*:\n",
        "\n",
        "$$\n",
        "\\theta = \n",
        "\\begin{bmatrix}\n",
        "    \\theta_0 \\\\\n",
        "    \\theta_1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "We will learn the best $\\theta$. \n",
        "\n",
        "A given $\\theta$ vector defines a line. What's the best line? We need the notion of a cost function to decide. \n",
        "\n",
        "<img src=\"https://github.com/thomouvic/SENG474/raw/main/images/hypothesis_line.png\" width=\"200\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZeS5jxcAccZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cost Function\n",
        "\n",
        "How to choose parameters $\\theta_0, \\theta_1$?\n",
        "\n",
        "Choose $\\theta_0, \\theta_1$ so that $h_{\\theta}(x)$ is close to $y$ for our training examples. \n",
        "\n",
        "$$\n",
        "\\min_{\\theta_0, \\theta_1} \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)})-y^{(i)}  \\right)^2\n",
        "$$\n",
        "\n",
        "This means: Find the values of $\\theta_0, \\theta_1$ that minimize the cost function. \n",
        "\n",
        "This cost function is also called Mean Squared Error (MSE). We will denote the cost function by $J(\\theta_0, \\theta_1)$. The variables in this function are $\\theta_0, θ_1$. $x$ and $y$ are data, i.e. constant.\n",
        "\n",
        "So, the cost function is:\n",
        "$$\n",
        "J(\\theta_0, \\theta_1) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)})-y^{(i)}  \\right)^2\n",
        "$$\n",
        "Sometimes, people add a 2 in the denominator: \n",
        "$$\n",
        "J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)})-y^{(i)}  \\right)^2\n",
        "$$\n",
        "Of course, the same $\\theta_0, \\theta_1$ that minimize the second version will also minimize the first version and vice-versa. This is only to make the formulas look nicer. \n",
        "\n",
        "## Cost function intuition\n",
        "\n",
        "Let's consider a simplified hypothesis where $θ_0 = 0$, so we have $h_{\\theta}(x) = \\theta_1 x$. The cost function becomes:\n",
        "\n",
        "$$\n",
        "J(\\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\theta_1 x^{(i)}-y^{(i)}  \\right)^2\n",
        "$$\n",
        "\n",
        "and we want to solve $\\min_{\\theta_1} J(\\theta_1)$.\n",
        "\n",
        "### Comparing $h_{\\theta}(x)$ and $J(\\theta_1)$\n",
        "\n",
        "<img src=\"https://github.com/thomouvic/SENG474/raw/main/images/hypothesis_vs_cost.png\" width=\"600\"/>\n",
        "</div>\n",
        "\n",
        "$h_{\\theta}(x)$, for fixed $\\theta_1$, is a function of $x$. In the left figure, each line corresponds to a $\\theta_1$. There are three training points. The line in the middle has $\\theta_1=1$. It makes zero error, i.e. $J(1)=0$. More precisely, \n",
        "\n",
        "$$\n",
        "J(\\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\theta_1 x^{(i)}-y^{(i)}  \\right)^2 = \\frac{1}{2*3}(0^2+0^2+0^2) = 0\n",
        "$$\n",
        "\n",
        "The lower line makes errors. It has $\\theta_1=0.5$. The cost for this line is: \n",
        "$$\n",
        "J(0.5) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\theta_1 x^{(i)}-y^{(i)}  \\right)^2 = \\frac{1}{2*3}((0.5-1)^2+(1-2)^2+(1.5-3)^2) = 0.58 \n",
        "$$ \n",
        "\n",
        "The uppper line also makes errors. It has $\\theta_1=1.5$. The cost for this line is also 0.58. \n",
        "\n",
        "**Exercise**: What is the cost value for $\\theta_1=0$?\n",
        "\n",
        "We plot these cost values on the chart on the right. Each line on the left becomes one point in the right. The shape of the graph in the right if we repeat the process for many lines in the left will be a parabola as shown there.\n",
        "\n",
        "---\n",
        "\n",
        "Now suppose $\\theta_0 \\neq 0$. We can plot $J(\\theta_0,\\theta_1)$ in 3D:\n",
        "\n",
        "<img src=\"https://github.com/thomouvic/SENG474/raw/main/images/cost_in_3d.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "Each point on the 3D surface corresponds to a line in 2D. The 3D surface has a bowl shape. For a fixed $\\theta_0$, we still get parabola as in the case when $\\theta_0=0$. \n",
        "\n",
        "Contour plots are a way to show three-dimensional data on a two-dimensional graph. They use lines or colors to represent different values of a third variable. \n",
        "\n",
        "<img src=\"https://github.com/thomouvic/SENG474/raw/main/images/countour.png\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "Each contour point corresponds to pair of $\\theta_0, \\theta_1$, i.e. to a line. \n",
        "\n",
        "All the points on a contour line have the same $J$ value. \n",
        "\n",
        "Point $\\theta_0 = 230, \\theta_1=0.15$ is right at the center of contour circles and is the point that minimizes $J(\\theta_0, \\theta_1)$."
      ],
      "metadata": {
        "id": "d39X47O4EIHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent\n",
        "\n",
        "Recall, we want to minimize $J(\\theta_0, \\theta_1)$. \n",
        "The idea of Gradient Descent (GD) algorithm is as follows.  \n",
        "\n",
        "*   Start with some random values for $\\theta_0, \\theta_1$, e.g. $\\theta_0=0, \\theta_1=0$.\n",
        "*   Keep changing $\\theta_0, \\theta_1$ to reduce $J(\\theta_0, \\theta_1)$ until we end up at a minimum.\n",
        "\n",
        "Specifically, we change $\\theta_0, \\theta_1$ in the opposite direction of the gradient, or the \"stepest slope\". \n",
        "\n",
        "<img src=\"https://github.com/thomouvic/SENG474/raw/main/images/gradient_descent.png\" width=\"550\"/>\n",
        "\n",
        "\n",
        "\n",
        "Formally, the GD algorithm is: \n",
        "\n",
        "\n",
        "\n",
        "*   Repeat until convergence\n",
        "    - $\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0,\\theta_1)$ for $j=0,1$\n",
        "\n",
        "$\\alpha$ is the *learning rate* (how big a step we take). It is also called $\\eta$ in other resources, such as the Geron's book. \n",
        "\n",
        "**Warning.** We need to simultaneously update $\\theta_0,\\theta_1$. \n",
        "\n",
        "**Correct:**\n",
        "\n",
        "temp0 := $\\theta_0 - \\alpha \\frac{\\partial}{\\partial \\theta_0} J(\\theta_0,\\theta_1)$\n",
        "\n",
        "temp1 := $\\theta_1 - \\alpha \\frac{\\partial}{\\partial \\theta_1} J(\\theta_0,\\theta_1)$\n",
        "\n",
        "$\\theta_0$ := temp0\n",
        "\n",
        "$\\theta_1$ := temp1\n",
        "\n",
        "\n",
        "**Incorrect:**\n",
        "\n",
        "temp0 := $\\theta_0 - \\alpha \\frac{\\partial}{\\partial \\theta_0} J(\\theta_0,\\theta_1)$\n",
        "\n",
        "$\\theta_0$ := temp0\n",
        "\n",
        "temp1 := $\\theta_1 - \\alpha \\frac{\\partial}{\\partial \\theta_1} J(\\theta_0,\\theta_1)$\n",
        "\n",
        "$\\theta_1$ := temp1\n",
        "\n",
        "---\n",
        "\n",
        "## Intutition about gradient\n",
        "\n",
        "Suppose again we look for a hypothesis $h_{theta} = \\theta_1 x$, i.e. where $\\theta_0 = 0$. So, we seek $\\min_{\\theta_1} J(\\theta_1)$. Since, we are in 1D, we write the update rule as: \n",
        "$$\n",
        "\\theta_1 := \\theta_1 - \\alpha \\frac{d}{d \\theta_1} J(\\theta_1)\n",
        "$$\n",
        "\n",
        "i.e. the partial derivative $\\partial$ becomes just ordinary derivative $d$.\n",
        "\n",
        "To see that the oposite of the derivative at each $\\theta_1$ point takes us in the right direction, suppose that we have the following parabola for $J(\\theta_1)$. The shape and position of the parabola depends on the given training set, but it is always a parabola for this hypothesis class. \n",
        "\n",
        "<img src=\"https://github.com/thomouvic/SENG474/raw/main/images/gradient_intuition.png\" width=\"300\"/>\n",
        "\n",
        "Suppose the current value of $\\theta_1$ is a point on the right side in the horizontal axis. The derivative of $J(\\theta_1)$ at this $\\theta_1$ value is the slope of the tangent line on the curve. The slope of the tangent line can be expressed as the ratio between the vertical change ($h$) and the horizontal change ($b$). The slope is positive in this case, so we have:\n",
        "\n",
        "$$\n",
        "\\theta_1 := \\theta_1 - \\alpha \\cdot (\\mbox{positive number})\n",
        "$$\n",
        "\n",
        "This is the right thing to do, as it takes us left, toward the minimum of $J$, i.e. $\\theta_1$ decreases.\n",
        "\n",
        "Now, suppose the current value of $\\theta_1$ is a point on the left side in the horizontal axis. The derivative of $J(\\theta_1)$ at this $\\theta_1$ value is the slope of the tangent line on the curve. The slope is negative in this case, so we have:\n",
        "\n",
        "$$\n",
        "\\theta_1 := \\theta_1 - \\alpha \\cdot (\\mbox{negative number})\n",
        "$$\n",
        "\n",
        "This is again the right thing to do, as it takes us right, toward the minimum of $J$, i.e. $\\theta_1$ inreases.\n",
        "\n",
        "### Intuition about $\\alpha$\n",
        "\n",
        "$$\n",
        "\\theta_1 := \\theta_1 - \\alpha \\frac{d}{d \\theta_1} J(\\theta_1)\n",
        "$$\n",
        "\n",
        "If $\\alpha$ too small, GD is slow. \n",
        "\n",
        "If $\\alpha$ too big, GD can overshoot the minimum and can fail to converge, or even diverge.\n",
        "\n",
        "<img src=\"https://github.com/thomouvic/SENG474/raw/main/images/alpha_intuition.png\" width=\"200\"/>\n",
        "\n",
        "\n",
        "Also, obeserve that steps will be smaller and smaller (i.e. more careful) as GD approaches minimum, so no need to decrease $\\alpha$ over time. At the minimum the gradient becomes 0.\n",
        "\n",
        "<img src=\"https://github.com/thomouvic/SENG474/raw/main/images/smaller_steps.png\" width=\"300\"/>"
      ],
      "metadata": {
        "id": "vYV2969GdOmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing the gradient\n",
        "\n",
        "Recall\n",
        "\n",
        "$$\n",
        "J(\\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)})-y^{(i)}  \\right)^2\n",
        "= \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\theta_0 + \\theta_1 x^{(i)}-y^{(i)}  \\right)^2\n",
        "$$\n",
        "\n",
        "We have \n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\theta_0} = \\frac{1}{2m}\\cdot 2 \\sum_{i=1}^{m}\\left( \\theta_0 + \\theta_1 x^{(i)}-y^{(i)}  \\right) = \n",
        "\\frac{1}{m}\\sum_{i=1}^{m}\\left( h_{\\theta}(x^{(i)})-y^{(i)}  \\right)\n",
        "$$\n",
        "and\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\theta_1} = \\frac{1}{2m}\\cdot 2 \\sum_{i=1}^{m}\\left( \\theta_0 + \\theta_1 x^{(i)}-y^{(i)}  \\right) x^{(i)} = \n",
        "\\frac{1}{m}\\sum_{i=1}^{m}\\left( h_{\\theta}(x^{(i)})-y^{(i)}  \\right) x^{(i)}\n",
        "$$\n",
        "\n",
        "Now the GD algorithm is given as follows. \n",
        "\n",
        "* Repeat until convergence\n",
        "  - $\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}\\left( h_{\\theta}(x^{(i)})-y^{(i)}  \\right)$\n",
        "  - $\\theta_1 := \\theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}\\left( h_{\\theta}(x^{(i)})-y^{(i)}  \\right) x^{(i)}$\n",
        "\n",
        "Again these updates in each iteration have to be simultaneous. \n",
        "\n",
        "This is called \"**batch**\" GD because all the training instances are used in each iteration. "
      ],
      "metadata": {
        "id": "OqRVVtQc3eHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression with Multiple Variables\n",
        "\n",
        "The input features are denoted by $x_1, x_2, x_3, x_4$, and so on. The target is denoted as before by $y$. The notation is as follows.\n",
        "\n",
        "$n$ is the number of features. \n",
        "\n",
        "$x$ is the vector of input features of a generic training example.\n",
        "\n",
        "$x_j$ is the value of feature j in a generic training example.\n",
        "\n",
        "$x^{(i)}$ is the vector of input features of the i-th training example.\n",
        "\n",
        "$x^{(i)}_j$ is the value of feature j in the i-th training example.\n",
        "\n",
        "Here is a partial housing dataset:\n",
        "\n",
        "| size ($x_1$) | #bedrooms ($x_2$)| #floors ($x_3$) | age ($x_4$)| price ($y$)|\n",
        "|--|--|--|--|--|\n",
        "|2104 | 5 | 1 | 45 | 460|\n",
        "|1416 | 3 | 2 | 40 | 232|\n",
        "|1534 | 3 | 2 | 30 | 315|\n",
        "|852  | 2 | 1 | 36 | 178|\n",
        "\n",
        "**e.g.** \n",
        "\n",
        "$x^{(2)} =  \n",
        "\\begin{bmatrix}\n",
        "     1416 & 3 & 2 & 40 & 232\n",
        "\\end{bmatrix}$\n",
        "\n",
        "$x^{(2)}_3 = 2$\n",
        "\n",
        "The hypothesis is now: \n",
        "$$\n",
        "h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\cdots + \\theta_n x_n\n",
        "$$\n",
        "\n",
        "e.g. for the housing example we could have: \n",
        "\n",
        "$$\n",
        "h_{\\theta}(x) = 80 + 0.1 x_1 + 0.01 x_2 + 3 x_3 -2 x_4\n",
        "$$\n",
        "\n",
        "Here, 80 is the base price. The final price goes up a little bit if the size is bigger ($x_1$), just a tiny bit up if there are more bedrooms ($x_2$), quite significantly up if there are more floors, and goes down if the house is older. \n",
        "\n",
        "For convenience, let's add a dummy attribute, $x_0$, to the training set that is set to 1, i.e. the training set becomes: \n",
        "\n",
        "|dummy ($x_0$)| size ($x_1$) | #bedrooms ($x_2$)| #floors ($x_3$) | age ($x_4$)| price ($y$)|\n",
        "|--|--|--|--|--|--|\n",
        "|1|2104 | 5 | 1 | 45 | 460|\n",
        "|1|1416 | 3 | 2 | 40 | 232|\n",
        "|1|1534 | 3 | 2 | 30 | 315|\n",
        "|1|852  | 2 | 1 | 36 | 178|\n",
        "\n",
        "Now, the hypothesis can be written as:\n",
        "\n",
        "$$\n",
        "h_{\\theta}(x) = \\theta_0 x_0 + \\theta_1 x_1 + \\cdots + \\theta_n x_n = x \\cdot \\theta\n",
        "$$\n",
        "\n",
        "Here we consider $x$ to be a $1 \\times n$ row matrix, and $\\theta$ to be a $n \\times 1$ column matrix. If $x$ is considered a column matrix instead, then the above is written as $\\theta^T \\cdot x$. Clearly, the result (scalar) is the same. "
      ],
      "metadata": {
        "id": "ApA7lXpW6dyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GD for Multivariate Regression\n",
        "\n",
        "To recap: \n",
        "\n",
        "Hypothesis: $h_{\\theta}(x) = \\theta_0 x_0 + \\theta_1 x_1 + \\cdots + \\theta_n x_n = x \\cdot \\theta$\n",
        "\n",
        "\n",
        "Parameters: $\\theta_0, \\theta_1, \\cdots, \\theta_n$\n",
        "\n",
        "Cost function: $J(\\theta) = J(\\theta_0, \\theta_1, \\cdots, \\theta_n) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)})-y^{(i)}  \\right)^2$\n",
        "\n",
        "GD Algorithm: \n",
        "\n",
        "*   Repeat until convergence\n",
        "    - $\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0,\\theta_1, \\ldots, \\theta_n)$, for $j=0,1,\\ldots,n$\n",
        "\n",
        "\n",
        "### Gradient computation\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial}{\\partial \\theta_j} J(\\theta_j) \n",
        "&= \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\theta_0 x^{(i)}_0 + \\theta_1 x^{(i)}_1 + \\cdots + \\theta_n x^{(i)}_n -y^{(i)}  \\right)^2 \\\\\n",
        "&=\\frac{1}{2m} 2 \\sum_{i=1}^{m} \\left( \\theta_0 x^{(i)}_0 + \\theta_1 x^{(i)}_1 + \\cdots + \\theta_n x^{(i)}_n -y^{(i)}  \\right) x^{(i)}_j \\\\\n",
        "&=\\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right) x^{(i)}_j \n",
        "\\end{align*}\n",
        "\n",
        "The GD algorithm is now written as:\n",
        "\n",
        "*   Repeat until convergence\n",
        "    - $\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right) x^{(i)}_j$, for $j=0,1,\\ldots,n$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ioNEFHRoH7Qx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GD Vectorization\n",
        "\n",
        "Instead of computing individual partial derivatives put them all together in the gradient vector. \n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla_{\\theta} J(\\theta) = \n",
        "\\begin{bmatrix}\n",
        "  \\frac{\\partial J(\\theta)}{\\partial\\theta_0} \\\\\n",
        "  \\frac{\\partial J(\\theta)}{\\partial\\theta_1} \\\\\n",
        "  \\vdots \\\\\n",
        "  \\frac{\\partial J(\\theta)}{\\partial\\theta_n}\n",
        "\\end{bmatrix} \n",
        "=\n",
        "\\frac{1}{m} \n",
        "\\begin{bmatrix}\n",
        "  \\sum_{i=1}^m \\left( x^{(i)}\\theta - y^{(i)} \\right)x^{(i)}_0 \\\\\n",
        "  \\sum_{i=1}^m \\left( x^{(i)}\\theta - y^{(i)} \\right)x^{(i)}_1 \\\\ \\\\\n",
        "  \\vdots \\\\\n",
        "  \\sum_{i=1}^m \\left( x^{(i)}\\theta - y^{(i)} \\right)x^{(i)}_n \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "Let's expand one of the sums above, e.g. let's expand the first one. The other ones are similar.  \n",
        "\n",
        "\\begin{align*}\n",
        "\\left( x^{(1)}\\theta - y^{(1)} \\right)x^{(1)}_0 +\n",
        "\\left( x^{(2)}\\theta - y^{(2)} \\right)x^{(2)}_0 +\n",
        "\\cdots +\n",
        "\\left( x^{(m)}\\theta - y^{(m)} \\right)x^{(m)}_0\n",
        "\\end{align*}\n",
        "\n",
        "This is equal to \n",
        "\\begin{align*}\n",
        "x_0^T \\cdot\n",
        "\\begin{bmatrix}\n",
        "  x^{(1)}\\theta - y^{(1)} \\\\\n",
        "  x^{(2)}\\theta - y^{(2)} \\\\\n",
        "  \\vdots \\\\\n",
        "  x^{(m)}\\theta - y^{(m)} \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "$x_0$ is column matrix, so $x_0^T$ is a row matrix. The matrix on the right can be written as\n",
        "\n",
        "\\begin{align*}\n",
        "\\begin{bmatrix}\n",
        "  x^{(1)}\\theta - y^{(1)} \\\\\n",
        "  x^{(2)}\\theta - y^{(2)} \\\\\n",
        "  \\vdots \\\\\n",
        "  x^{(m)}\\theta - y^{(m)} \\\\\n",
        "\\end{bmatrix}\n",
        "&=\n",
        "\\begin{bmatrix}\n",
        "  x^{(1)}\\theta \\\\\n",
        "  x^{(2)}\\theta \\\\\n",
        "  \\vdots \\\\\n",
        "  x^{(m)}\\theta \\\\\n",
        "\\end{bmatrix} - y \\\\\n",
        "\\\\\n",
        "&=\n",
        "X\\cdot \\theta - y\n",
        "\\end{align*}\n",
        "\n",
        "Going back to the sum, we have\n",
        "\\begin{align*}\n",
        "\\sum_{i=1}^m \\left( x^{(i)}\\theta - y^{(i)} \\right)x^{(i)}_0 = x_0^T \\cdot (X\\cdot \\theta - y) \n",
        "\\end{align*}\n",
        "\n",
        "It's very similar for the other sums, so, we can write \n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla_{\\theta} J(\\theta) = \n",
        "\\frac{1}{m} \n",
        "\\begin{bmatrix}\n",
        "  x_0^T (X \\theta - y) \\\\\n",
        "  x_1^T (X \\theta - y) \\\\ \\\\\n",
        "  \\vdots \\\\\n",
        "  x_n^T (X \\theta - y) \\\\\n",
        "\\end{bmatrix} \n",
        "= \n",
        "\\frac{1}{m} X^T(X\\theta - y)\n",
        "\\end{align*}\n",
        "\n",
        "Now, $\\frac{1}{m} X^T(X\\theta - y)$ is a neat, fully vectorized expression, which we can easily plug in the update equation of GD. "
      ],
      "metadata": {
        "id": "HWJCt-5crQV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normal Equations\n",
        "\n",
        "Another way to find the best $\\theta$ vector is analytically. If you remember from calculus, a differentiable function has a mimimum at a point where the derivative (or gradient in more than 1D) becomes zero. So, let's take the gradient vector from above, set it zero, and solve for $\\theta$.\n",
        "\n",
        "\\begin{align*}\n",
        "X^T(X\\theta - y) &= 0 \\\\\n",
        "X^TX\\theta - X^Ty &= 0 \\\\\n",
        "X^TX\\theta &= X^Ty \\\\\n",
        "\\theta &= (X^TX)^{-1}X^Ty\n",
        "\\end{align*}\n",
        "\n",
        "The last line above is called the **normal equation**. \n",
        "\n",
        "$X$ is $m \\times (n+1)$. Recall we added the dummy feature, that's why $n+1$, not $n$.\n",
        "\n",
        "$X^T$ is $(n+1) \\times m$.\n",
        "\n",
        "$X^TX$ is $(n+1) \\times (n+1)$\n",
        "\n",
        "$(X^TX)^{-1}$ is $(n+1) \\times (n+1)$, inversion is a cubic-time operation, good if the number of features is not big\n",
        "\n",
        "$(X^TX)^{-1}X^T$ is $(n+1) \\times m$\n",
        "\n",
        "$y$ is $m \\times 1$\n",
        "\n",
        "$(X^TX)^{-1}X^Ty$ is $(n+1) \\times 1$, exactly the dimensions of $\\theta$, which is a column matrix of $n+1$ elements. \n",
        "\n",
        "Here is a Python program that computes the normal equation:"
      ],
      "metadata": {
        "id": "UPk1njwe0KGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X=np.array([\n",
        "    [1,90,1], \n",
        "    [1,80,3],\n",
        "    [1,90,2], \n",
        "    [1,70,8]])\n",
        "\n",
        "y=np.array([[50],[60],[55],[70]]);\n",
        "\n",
        "theta = np.linalg.inv(X.T @ X)  @ X.T @ y\n",
        "theta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x_j3TIhBiVX",
        "outputId": "df6d3d57-3af2-4d4a-fdc9-0014fc3df313"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[86.5],\n",
              "       [-0.4],\n",
              "       [ 1.5]])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GD vs. normal equation\n",
        "\n",
        "**GD:**\n",
        "Needs to iterate, sometimes a lot.\n",
        "Need to play with kappa.\n",
        "\n",
        "Method of choice when $m$ is big.\n",
        "\n",
        "Also, it's a general method applicable for other functions for which we don't have normal equations. \n",
        "\n",
        "\n",
        "**Normal equation:**\n",
        "No need to iterate, adjust kappa, or scale attributes.\n",
        "\n",
        "However, the challenge is to compute: $(X^TX)^{-1}$. \n",
        "\n",
        "$X^TX$ not a problem; result is $(m+1) \\times (m+1)$.\n",
        "Inverting takes $\\approx O(m^3)$ time.\n",
        "Difficult for big $m$."
      ],
      "metadata": {
        "id": "hgK8XUNmCF4C"
      }
    }
  ]
}