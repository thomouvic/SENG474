{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuhwew43+8kklilndQTUhL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomouvic/SENG474/blob/main/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression with One Variable\n",
        "\n",
        "**Example:** Housing Prices (Portland, OR)\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/thomouvic/SENG474/raw/main/images/housing.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "**Supervised Learning:**\n",
        "Given the \"right answer\" for each example in data (training instance), predict real-valued output (**regression**), or discrete-valued output (**classification**). \n",
        "\n",
        "**Formally:**\n",
        "\n",
        "Training set:\n",
        "\n",
        "| size (x) | price (y)  |\n",
        "|--|--|\n",
        "| 2104 | 460 |\n",
        "| 1416 | 232 |\n",
        "| 1534 | 315 |\n",
        "\n",
        "**Notation:**\n",
        "\n",
        "$m$: number of training examples (instances)\n",
        "\n",
        "$x$: input variable/feature\n",
        "\n",
        "$y$: output variable / target variable / label variable\n",
        "\n",
        "$(x,y)$: a generic training example\n",
        "\n",
        "$\\left( x^{(i)}, y^{(i)} \\right)$: the i-th training example.\n",
        "\n",
        "\n",
        "**E.g.**\n",
        "\n",
        "$x^{(1)} = 2104$\n",
        "\n",
        "$x^{(2)} = 1416$\n",
        "\n",
        "$y^{(1)} = 460$\n",
        "\n",
        "$y^{(2)} = 232$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/thomouvic/SENG474/raw/main/images/hypothesis.png\" width=\"300\"/>\n",
        "</div>\n",
        "\n",
        "$h$ maps from x's to y's.\n",
        "\n",
        "How to represent $h$?\n",
        "\n",
        "$$\n",
        "h_{\\theta}(x) = \\theta_0 + \\theta_1 x\n",
        "$$\n",
        "\n",
        "$\\theta$ is the parameter vector represented as a *column matrix*:\n",
        "\n",
        "$$\n",
        "\\theta = \n",
        "\\begin{bmatrix}\n",
        "    \\theta_0 \\\\\n",
        "    \\theta_1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "We will learn the best $\\theta$. \n",
        "\n",
        "A given $\\theta$ vector defines a line. What's the best line?\n",
        "\n",
        "<img src=\"https://github.com/thomouvic/SENG474/raw/main/images/hypothesis_line.png\" width=\"200\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Cost Function\n",
        "\n",
        "How to choose parameters $\\theta_0, \\theta_1$?\n",
        "\n",
        "Choose $\\theta_0, \\theta_1$ so that $h_{\\theta}(x)$ is close to $y$ for our training examples. \n",
        "\n",
        "$$\n",
        "\\min_{\\theta_0, \\theta_1} \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)})-y^{(i)}  \\right)^2\n",
        "$$\n",
        "\n",
        "This means: Find the values of $\\theta_0, \\theta_1$ that minimize the cost function. \n",
        "\n",
        "This cost function is also called Mean Squared Error (MSE). We will denote the cost function by $J(\\theta_0, \\theta_1)$. The variables in this function are $\\theta_0, θ_1$. $x$ and $y$ are data, i.e. constant.\n",
        "\n",
        "So, the cost function is:\n",
        "$$\n",
        "J(\\theta_0, \\theta_1) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)})-y^{(i)}  \\right)^2\n",
        "$$\n",
        "Sometimes, people add a 2 in the denominator: \n",
        "$$\n",
        "J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)})-y^{(i)}  \\right)^2\n",
        "$$\n",
        "Of course, the same $\\theta_0, \\theta_1$ that minimize the second version will also minimize the first version and vice-versa. This is only to make the formulas look nicer. \n",
        "\n",
        "## Cost function intuition\n",
        "\n",
        "Let's consider a simplified hypothesis where $θ_0 = 0$, so we have $h_{\\theta}(x) = \\theta_1 x$. The cost function is:\n",
        "\n",
        "$$\n",
        "J(\\theta_1) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\theta_1 x^{(i)}-y^{(i)}  \\right)^2\n",
        "$$\n",
        "\n",
        "and we want to solve $\\min_{\\theta_1} J(\\theta_1)$.\n",
        "\n",
        "### Comparing $h_{\\theta}(x)$ and $J(\\theta_1)$\n",
        "\n",
        "<img src=\"https://github.com/thomouvic/SENG474/raw/main/images/hypothesis_vs_cost.png\" width=\"600\"/>\n",
        "</div>\n",
        "\n",
        "$h_{\\theta}(x)$, for fixed $\\theta_1$, is a function of $x$. In the left figure, each line corresponds to a $\\theta_1$. There are three training points. The line in the middle has $\\theta_1=1$. It makes zero error, i.e. $J(1)=0$. More precisely, \n",
        "\n",
        "$$\n",
        "J(\\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\theta_1 x^{(i)}-y^{(i)}  \\right)^2 = \\frac{1}{2*3}(0^2+0^2+0^2) = 0\n",
        "$$\n",
        "\n",
        "The lower line makes errors. It has $\\theta_1=0.5$. The cost for this line is: \n",
        "$$\n",
        "J(0.5) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\theta_1 x^{(i)}-y^{(i)}  \\right)^2 = \\frac{1}{2*3}((0.5-1)^2+(1-2)^2+(1.5-3)^2) = 0.58 \n",
        "$$ \n",
        "\n",
        "The uppper line also makes errors. It has $\\theta_1=1.5$. The cost for this line is also 0.58. \n",
        "\n",
        "Exercise: What is the cost value for $\\theta_1=0$?\n",
        "\n",
        "We plot these cost values on the chart on the right. Each line on the left becomes one point in the right. The shape of the graph in the right if we repeat the process for many lines in the left will be a parabola as shown there.\n",
        "\n",
        "---\n",
        "\n",
        "Now suppose $\\theta_0 \\neq 0$. We can plot $J()$ in 3D:\n",
        "\n",
        "<img src=\"https://github.com/thomouvic/SENG474/raw/main/images/cost_in_3d.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "Each point on the 3D surface corresponds to a line in 2D. The 3D surface has a bowl shape. For a fixed $\\theta_0$, we still get parabola as in the case when $\\theta_0=0$. \n",
        "\n",
        "Contour plots are a way to show three-dimensional data on a two-dimensional graph. They use lines or colors to represent different values of a third variable. \n",
        "\n",
        "<img src=\"https://github.com/thomouvic/SENG474/raw/main/images/countour.png\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "Each contour point corresponds to pair of $\\theta_0, \\theta_1$, i.e. to a line. \n",
        "\n",
        "All the points on a contour line have the same $J$ value. \n",
        "\n",
        "Point $\\theta_0 = 230, \\theta_1=0.15$ is right at the center of contour circles and is the point that minimizes $J(\\theta_0, \\theta_1)$.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZeS5jxcAccZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u1nkObFStZaR"
      }
    }
  ]
}